{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8a1f14",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|---|---|\n",
    "|1. multiface detection을 위한 widerface 데이터셋의 전처리가 적절히 진행되었다.|tfrecord 생성, augmentation, prior box 생성 등의 과정이 정상적으로 진행되었다.|\n",
    "|2. SSD 모델이 안정적으로 학습되어 multiface detection이 가능해졌다.|inference를 통해 정확한 위치의 face bounding box를 detect한 결과이미지가 제출되었다.|\n",
    "|3. 이미지 속 다수의 얼굴에 스티커가 적용되었다.|이미지 속 다수의 얼굴의 적절한 위치에 스티커가 적용된 결과이미지가 제출되었다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbef7e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ff33b12",
   "metadata": {},
   "source": [
    "x0, y0, w, h, blur, expression, illumination, invalid, occlusion, pose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916bd6a",
   "metadata": {},
   "source": [
    "필요한 라이브러리와 전역 변수를 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "232f457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, time\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "DATA_PATH = os.path.join(PROJECT_PATH, 'widerface')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
    "TRAIN_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'train_mask.tfrecord')\n",
    "VALID_TFRECORD_PATH = os.path.join(PROJECT_PATH, 'dataset', 'val_mask.tfrecord')\n",
    "CHECKPOINT_PATH = os.path.join(PROJECT_PATH, 'checkpoints')\n",
    "\n",
    "DATASET_LEN = 12880\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_WIDTH = 320\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_LABELS = ['background', 'face']\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093c953",
   "metadata": {},
   "source": [
    "tfrecord 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d436a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b3831c",
   "metadata": {},
   "source": [
    "augmentation 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66fa394",
   "metadata": {},
   "source": [
    "prior box 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSD 모델이 안정적으로 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eea1e9",
   "metadata": {},
   "source": [
    "multiface detection - inference를 통해 정확한 위치의 face bounding box를 detect한 결과이미지가 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f3cea",
   "metadata": {},
   "source": [
    "이미지 속 다수의 얼굴의 적절한 위치에 스티커가 적용된 결과이미지 제출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4567ffe1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "934fcb18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49432519",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4a8b337",
   "metadata": {},
   "source": [
    "utils - bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc70a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_parser import (\n",
    "    get_box as parse_box, parse_widerface, \n",
    "    process_image, xywh_to_voc,\n",
    "    make_example\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h\n",
    "\n",
    "def parse_widerface(file):\n",
    "    \"\"\"이미지별 bounding box 정보를 wider_face_train_bbx_gt.txt에서 파싱해서 리스트로 추출\"\"\"\n",
    "    infos = []\n",
    "    with open(file) as fp:\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            n_object = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(n_object):\n",
    "                box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = parse_box(box)\n",
    "                if (w == 0) or (h == 0):\n",
    "                    continue\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if n_object == 0:\n",
    "                box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = parse_box(box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            infos.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "    return infos\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83097da3",
   "metadata": {},
   "source": [
    "추출된 정보를 실제 이미지 정보와 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e041c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_file):\n",
    "    \"\"\"추출된 정보를 실제 이미지 정보와 결합\"\"\"\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        return 1, image_string, None\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137444e5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97109e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    \"\"\"[x, y, w, h] 형태를 [x_min, y_min, x_max, y_max] 형태의 꼭짓점 좌표 정보로 변환\"\"\"\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4311454",
   "metadata": {},
   "source": [
    "이미지 입력 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(DATA_PATH, 'wider_face_split', 'wider_face_train_bbx_gt.txt')\n",
    "for i, info in enumerate(parse_widerface(file_path)):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(DATA_PATH, 'WIDER_train', 'images', info[0])\n",
    "    _, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9747b1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e84466e",
   "metadata": {},
   "source": [
    "---\n",
    "### TFRecord 생성\n",
    "TFRecord는 여러 개의 tf.train.Example로 이루어져 있고, 한 개의 tf.train.Example은 여러 개의 tf.train.Feature로 이루어져 있습니다.\n",
    "\n",
    "데이터의 단위를 이루는 tf.train.Example 인스턴스를 생성하는 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f389e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(image_string, image_infos):\n",
    "    for info in image_infos:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    \n",
    "    return example\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val']:\n",
    "    if split == 'train':\n",
    "        output_file = TRAIN_TFRECORD_PATH \n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train'\n",
    "    else:\n",
    "        output_file = VALID_TFRECORD_PATH\n",
    "        anno_txt = 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_val'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(DATA_PATH, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(DATA_PATH, file_path, 'images', info[0])\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "                writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be6cd52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_mask.tfrecord  val_mask.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/aiffel/face_detector/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b82e8",
   "metadata": {},
   "source": [
    "#### Default box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2707c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_MIN_SIZES = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]\n",
    "BOX_STEPS = [8, 16, 32, 64]\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d120e245",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_box():\n",
    "    image_sizes = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    min_sizes = BOX_MIN_SIZES\n",
    "    steps= BOX_STEPS\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps\n",
    "    ]\n",
    "    boxes = []\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                s_kx = min_size / image_sizes[1]\n",
    "                s_ky = min_size / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                boxes += [cx, cy, s_kx, s_ky]\n",
    "    boxes = np.asarray(boxes).reshape([-1, 4])\n",
    "    return boxes\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3e02f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b7001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1)):\n",
    "    block_id = (tf.keras.backend.get_uid())\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)\n",
    "\n",
    "print('슝=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eca6a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4051a789",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "954d83fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41bc4e32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5160c9ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77701965",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cf0867",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
